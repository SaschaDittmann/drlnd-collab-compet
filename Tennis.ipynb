{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor & Critic Models\n",
    "Neural network architectures\n",
    "- Actor\n",
    "  - Input layer (size 8)\n",
    "  - FC layer (size 256)\n",
    "  - Relu\n",
    "  - Batch Normalization\n",
    "  - FC layer (size 128)\n",
    "  - Relu\n",
    "  - Output layer (size 2)\n",
    "  - Tanh\n",
    "- Critic\n",
    "  - Input layer (size 8)\n",
    "  - FC layer (size 256)\n",
    "  - Relu\n",
    "  - Batch Normalization\n",
    "  - FC layer (size 128)\n",
    "  - Relu\n",
    "  - Output layer (size 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Allocated:\", round(torch.cuda.memory_allocated(0)/1024**3,1), \"GB\")\n",
    "else:\n",
    "    print(\"Training on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = F.relu(self.fcs1(state))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Deep Deterministic Policy Gradients (DDPG) Agent\n",
    "\n",
    "The Deep Deterministic Policy Gradients (DDPG) algorithm below successfully solves the task of this project. It learns from the provided environment without any prior knowledge of it or data labels and maximizes reward by interacting with the environment.\n",
    "\n",
    "![DDPG Pseudocode](images/ddpg_pseudocode.png)\n",
    "(description courtesy of https://arxiv.org/pdf/1509.02971.pdf, see page 5)\n",
    "\n",
    "Based on this implementation, adaptations were needed in the model and Noise process to adapt from one to multiple agents. Moreover, the learning rate of the critic was reduced to 1e-4 which lead to faster and more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None])).float()\n",
    "        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.stack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        if use_cuda:\n",
    "            states = states.cuda()\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            next_states = next_states.cuda()\n",
    "            dones = dones.cuda()\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, index, nb_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        if use_cuda:\n",
    "            self.index = index = torch.tensor([index]).cuda()\n",
    "        else:\n",
    "            self.index = index = torch.tensor([index])\n",
    "            \n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed)\n",
    "        if use_cuda:\n",
    "            self.actor_local = self.actor_local.cuda()\n",
    "            self.actor_target = self.actor_target.cuda()\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(nb_agents * state_size, nb_agents * action_size, random_seed)\n",
    "        self.critic_target = Critic(nb_agents * state_size, nb_agents * action_size, random_seed)\n",
    "        if use_cuda:\n",
    "            self.critic_local = self.critic_local.cuda()\n",
    "            self.critic_target = self.critic_target.cuda()\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "        \n",
    "        self.soft_update(self.critic_local, self.critic_target, 1)\n",
    "        self.soft_update(self.actor_local, self.actor_target, 1)\n",
    "\n",
    "    def act(self, state, i_episode=0, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        if use_cuda:\n",
    "            state = torch.from_numpy(state).float().cuda()\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float()\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma, actions_target, actions_pred):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        rewards = rewards.unsqueeze(-1)\n",
    "        dones = dones.unsqueeze(-1)\n",
    "        \n",
    "        # ---------------------------- update critic ----------------------------\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        if use_cuda:\n",
    "            actions_target = torch.cat(actions_target, dim=1).cuda()\n",
    "        else:\n",
    "            actions_target = torch.cat(actions_target, dim=1)\n",
    "        Q_targets_next = self.critic_target(next_states.reshape(next_states.shape[0], -1), actions_target.reshape(next_states.shape[0], -1))\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards.index_select(1, self.index).squeeze(1) + (gamma * Q_targets_next * (1 - dones.index_select(1, self.index).squeeze(1)))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states.reshape(states.shape[0], -1), actions.reshape(actions.shape[0], -1))\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        if use_cuda:\n",
    "            actions_pred = torch.cat(actions_pred, dim=1).cuda()\n",
    "        else:\n",
    "            actions_pred = torch.cat(actions_pred, dim=1)\n",
    "        actor_loss = -self.critic_local(states.reshape(states.shape[0], -1), actions_pred.reshape(actions_pred.shape[0], -1)).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()                      \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Multi-Agent\n",
    "\n",
    "The Multi-Agent Deep Deterministic Policy Gradients (MADDPG) algorithm below successfully solves the task of this project. It learns from the provided environment without any prior knowledge of it or data labels and maximizes reward by interacting with the environment.\n",
    "\n",
    "This algorithm trains two separate agents to take actions based on their own observations as well as centralized critics with additional information of all agents. This algorithms builds on the concept of DDPG and brings it to multi-agent tasks by separating out their observations and thereby avoiding the apparent non-stationarity from the perspective of any individual agent in multi-agent environments. This approach does not contain the shortcoming of policy gradient algorithms, like REINFORCE, which exhibit high variance gradient estimates that are cumulating exponentially with the number of agents (leading to exponential decrease of the probability of taking a gradient step in the right direction), and render them unsuitable for problems with a large number of agents.\n",
    "\n",
    "The official paper can be found here: https://arxiv.org/pdf/1706.02275.pdf and the official repo can be found here: https://github.com/openai/maddpg. The paper is exploring deep RL methods for multi-agent domains and introduces MADDPG to solve (among other challenges) the non-stationarity of single agent's environments.\n",
    "\n",
    "The MADDPG in this repo is an off-policy multi-agent actor-critic approach that uses the concept of target networks with centralized training and decentralized execution. In the reference paper it is used for mixed cooperative-competitive environments.\n",
    "\n",
    "If we dissect these terms, this means:\n",
    "\n",
    "- Off-policy means that the agent updates its network using the expected return assuming a greedy policy will be followed (while on-policy approaches estimate the value of a policy while using it for control)\n",
    "- Multi-agent means that we are working with a system with more than one agent (in this case 2). This leads to interactions between agents, introducing more complexity and needs for novel approaches\n",
    "- Actor-critic means that the algorithm trains two networks at the same time for function approximation, where the actor learns the policy function mu which returns the optimal action(s) and the critic learns the value function to evaluate the actor's actions and helps improve the training ability\n",
    "- Target networks means the idea of creating a local and a target network to break the correlation between the target from the actions and thereby stabilize the learning\n",
    "- Mixed cooperative-competitive means a mixture of cooperative and competitive environments. In cooperative environments, the agents are only concerned about a group task with an across-agent reward. In a competitive environment, each agents is only concered about their own respective reward, where one agent's loss could be the other agent's gain (e.g., a game of two agents playing soccer against each other).\n",
    "- Centralized training and decentralized execution means that extra information is used by the critic compared to the actor, like states observed and agents taken by other agents. The actors only have access to their own observations and actions:\n",
    "\n",
    "![Centralized training and Decentralized execution](images/centralized_training.png)\n",
    "\n",
    "In general, the approach considers a game with N agents with policies parametrized by Theta_1 to Theta_N and policies pi_1 to pi_N for all agents. The gradient of the expected return for agent i, is then, given...\n",
    "\n",
    "- simple gradient update, which directly adjusts the policy parameters theta to maximize the objective function J\n",
    "- where state s is assumed via greedy policy mu and the actions a_i come from policy pi_i,\n",
    "- and Q_pi_i = Q_mu_i being the centralized action-value function, as explained above, that helps to consider extra information from other agents\n",
    "- and an experience replay buffer D\n",
    "- and working with N deterministic continuous policies mu_theta_i \n",
    "\n",
    "the gradient can be formulated as \n",
    "\n",
    "![the gradient](images/gradient.png)\n",
    "\n",
    "with the centralized action-value function being updated as\n",
    "\n",
    "![centralized action-value function](images/action_value_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DDPG agent contains the following components and configs:\n",
    "- A replay buffer to store memories with the size of 1e5\n",
    "- Minibatch sizes of 256\n",
    "- A discount factor of 0.99 for value function approximation\n",
    "- A soft update to blend the regular into the target network of 1e-3\n",
    "- Learning rates of the actor and critic each set to 1e-4\n",
    "- Noise according to the Ornstein-Uhlenbeck process with theta=0.15, sigma=0.2\n",
    "- Repetitions of learning per agent-step of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "UPDATE_EVERY = 2        # Udpate every\n",
    "LEARNING_REPS = 3       # Learning repetitions per step\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgents():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        self.ma = [Agent(state_size, action_size, i, n_agents, random_seed) for i in range(n_agents)]\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if len(self.memory) > BATCH_SIZE and self.t_step == 0:\n",
    "            for _ in range(LEARNING_REPS):\n",
    "                for agent in self.ma:\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences, agent, GAMMA)\n",
    "                    \n",
    "                for agent in self.ma:\n",
    "                    agent.soft_update(agent.critic_local, agent.critic_target, TAU)\n",
    "                    agent.soft_update(agent.actor_local, agent.actor_target, TAU)    \n",
    "              \n",
    "    def learn(self, experiences, agent, gamma):\n",
    "        states, actions, _, _, _ = experiences\n",
    "\n",
    "        if use_cuda:\n",
    "            actions_target = [agent_j.actor_target(states.index_select(1, torch.tensor([j]).cuda()).squeeze(1)) for j, agent_j in enumerate(self.ma)]\n",
    "        else:\n",
    "            actions_target = [agent_j.actor_target(states.index_select(1, torch.tensor([j])).squeeze(1)) for j, agent_j in enumerate(self.ma)]\n",
    "        \n",
    "        agent_action_pred = agent.actor_local(states.index_select(1, agent.index).squeeze(1))\n",
    "        if use_cuda:\n",
    "            actions_pred = [agent_action_pred if j==agent.index.cpu().numpy()[0] else actions.index_select(1, torch.tensor([j]).cuda()).squeeze(1) for j, agent_j in enumerate(self.ma)]\n",
    "        else:\n",
    "            actions_pred = [agent_action_pred if j==agent.index.numpy()[0] else actions.index_select(1, torch.tensor([j])).squeeze(1) for j, agent_j in enumerate(self.ma)]\n",
    "        agent.learn(experiences, gamma, actions_target, actions_pred)\n",
    "\n",
    "    def act(self, states, i_episode=0, add_noise=True):\n",
    "        actions = [np.squeeze(agent.act(np.expand_dims(state, axis=0), i_episode, add_noise), axis=0) for agent, state in zip(self.ma, states)]\n",
    "        return np.stack(actions)     \n",
    "        \n",
    "    def reset(self):\n",
    "        for agent in self.ma:\n",
    "            agent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agents\n",
    "m_agents = MultiAgents(state_size=state_size, action_size=action_size, n_agents=num_agents, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=10000, print_every=100):\n",
    "    # Create deque to track last 100 scores and scores for full list for plot\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] #activate training\n",
    "        states = env_info.vector_observations  \n",
    "        m_agents.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        while True:\n",
    "            actions = m_agents.act(states, i_episode, add_noise=True) # get MA-actions with noise\n",
    "            env_info = env.step(actions)[brain_name]   \n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "\n",
    "            m_agents.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            score += rewards\n",
    "            if any(dones):\n",
    "                break \n",
    "                \n",
    "        scores_deque.append(np.max(score))\n",
    "        scores.append(np.max(score))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.3f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if (np.mean(scores_deque) >= 0.5):\n",
    "            print('\\nSolved in {:d} episodes, with an average score over the last 100 episodes of: {:.2f}'.format(i_episode - print_every, np.mean(scores_deque)))\n",
    "            # Save final agent weights\n",
    "            for i, agent in enumerate(m_agents.ma):\n",
    "                torch.save(agent.actor_local.state_dict(), f'checkpoint_actor_{i}.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), f'checkpoint_critic_{i}.pth')\n",
    "            break\n",
    "        if (i_episode == n_episodes):\n",
    "            print('\\nNot solved in {:d} episodes. Achieved average score over the last 100 episodes of: {:.2f}'.format(i_episode - print_every, np.mean(scores_deque)))\n",
    "            # Save final agent weights\n",
    "            for i, agent in enumerate(m_agents.ma):\n",
    "                torch.save(agent.actor_local.state_dict(), f'unsolved_checkpoint_actor_{i}.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), f'unsolved_checkpoint_critic_{i}.pth')\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot scores of the learning trajectory\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score per episode\", c='gray')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.plot(pd.Series(scores).rolling(100).mean(), label=\"Score avg last 100 episodes\", c='blue')\n",
    "plt.axhline(0.05, c='g', ls = '--', label='Required avg score over 100 episodes')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0, 1.3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch your Trained Agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model - agent 0\n",
    "actor_state_dict_0 = torch.load('checkpoint_actor_0.pth')\n",
    "print(actor_state_dict_0.keys())\n",
    "m_agents.ma[0].actor_local.load_state_dict(actor_state_dict_0)\n",
    "\n",
    "critic_state_dict_0 = torch.load('checkpoint_critic_0.pth')\n",
    "print(critic_state_dict_0.keys())\n",
    "m_agents.ma[0].critic_local.load_state_dict(critic_state_dict_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model - agent 1\n",
    "actor_state_dict_1 = torch.load('checkpoint_actor_1.pth')\n",
    "print(actor_state_dict_1.keys())\n",
    "m_agents.ma[1].actor_local.load_state_dict(actor_state_dict_1)\n",
    "\n",
    "critic_state_dict_1 = torch.load('checkpoint_critic_1.pth')\n",
    "print(critic_state_dict_1.keys())\n",
    "m_agents.ma[1].critic_local.load_state_dict(critic_state_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = m_agents.act(states, add_noise=False) # select an action (for each agent)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "- Test implementations of other modern multi-agent approaches, e.g.\n",
    "  - MAA2C\n",
    "  - IQL\n",
    "  - IDDPG\n",
    "  - IPPO\n",
    "- Tune hyperparameters to accelerate training"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf-gpu.1-15.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
